{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='banana_app/Banana.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 37\n",
      "The state for the first agent looks like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contributions to \"Deep Reinforcement Learning Hands-On\" by Maxim Lapan\n",
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n",
    "            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        func = lambda x: torch.sign(x) * torch.sqrt(torch.abs(x))\n",
    "        self.epsilon_weight = func(torch.randn(self.in_features)) * func(torch.randn(self.out_features, 1))\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias = func(torch.randn(self.out_features, ))\n",
    "            bias = bias + self.sigma_bias * self.epsilon_bias.data\n",
    "        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight.data, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contributions to \"Deep Reinforcement Learning Hands-On\" by Maxim Lapan\n",
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n",
    "            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * self.epsilon_bias.data\n",
    "        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight.data, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, seed, state_size = 37, action_size = 4, fc1_units = 64, fc2_units = 64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = NoisyLinear(state_size, fc1_units)\n",
    "        self.fc2 = NoisyLinear(fc1_units, fc2_units)\n",
    "        self.fc2a = NoisyLinear(fc2_units, 1)\n",
    "        self.fc3 = NoisyLinear(fc1_units, fc2_units)\n",
    "        self.fc3a = NoisyLinear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        value = F.relu(self.fc2(x))\n",
    "        value = self.fc2a(value)\n",
    "        advantage = F.relu(self.fc3(x))\n",
    "        advantage = self.fc3a(advantage)\n",
    "        if len(advantage) == 4:\n",
    "            q_value = value + advantage - advantage.mean(0).unsqueeze(0)\n",
    "        else:\n",
    "            q_value = value + advantage - advantage.mean(1).unsqueeze(1)\n",
    "        \n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "STEP_SIZE = 1\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, seed, state_size = 37, action_size = 4, buffer_size = BUFFER_SIZE, batch_size = BATCH_SIZE):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.qnetwork_local = QNetwork(seed, state_size, action_size)\n",
    "        self.qnetwork_target = QNetwork(seed, state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr = LR)\n",
    "        \n",
    "        self.t_step = 0\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.memory_n = deque(maxlen = STEP_SIZE)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"R\"])\n",
    "        self.experience_n = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        return np.argmax(action_values.cpu().data.numpy())\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones, Rs = experiences\n",
    "        action_next = torch.argmax(self.qnetwork_local(torch.FloatTensor(next_states)).detach(), dim = 1).unsqueeze(1)\n",
    "        Q_target_next = self.qnetwork_target(torch.FloatTensor(next_states)).gather(1, action_next)\n",
    "        Q_targets = Rs + (gamma ** STEP_SIZE) * Q_target_next * (1 - dones)\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "                \n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience_n(state, action, reward, next_state, done)\n",
    "        self.memory_n.append(e)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in self.memory_n if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in self.memory_n if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in self.memory_n if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in self.memory_n if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in self.memory_n if e is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        for i in reversed(range(len(states))):\n",
    "            if i == len(states) - 1:\n",
    "                R = rewards[i]\n",
    "            else:\n",
    "                R = rewards[i] + GAMMA * R\n",
    "            \n",
    "        e = self.experience(states[i], actions[i], rewards[i], next_states[i], dones[i], R)\n",
    "        self.memory.append(e)     \n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        Rs = torch.from_numpy(np.vstack([e.R for e in experiences if e is not None])).float()\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones, Rs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tAverage Score: -0.40\n",
      "Episode 10\tAverage Score: -0.10\n",
      "Episode 15\tAverage Score: 0.27\n",
      "Episode 20\tAverage Score: 0.40\n",
      "Episode 25\tAverage Score: 0.80\n",
      "Episode 30\tAverage Score: 1.00\n",
      "Episode 35\tAverage Score: 0.94\n",
      "Episode 40\tAverage Score: 1.10\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes = 5000, max_t = 1000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        if i_episode % 5 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 30:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
